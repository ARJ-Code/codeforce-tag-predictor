{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt y preprocesamiento inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/codeforces_processed_data.csv')\n",
    "\n",
    "tags = df['tags'].apply(ast.literal_eval)\n",
    "all_tags = set([tag for sublist in tags for tag in sublist])\n",
    "\n",
    "df=df.head(100)\n",
    "\n",
    "def prompt(description):\n",
    "    all_tags_str = ', '.join(all_tags)\n",
    "    return f'Give this set of {all_tags_str} tags and this problem ${description}, give me the set of problem tags in the following format: greedy, implementation, dp'\n",
    "    \n",
    "df['prompt'] = df['description'].apply(prompt)\n",
    "\n",
    "df.to_csv('../data/codeforce_chatgpt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar las respuestas de chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/codeforce_chatgpt.csv')\n",
    "\n",
    "tags = df['tags'].apply(ast.literal_eval)\n",
    "all_tags = set([tag for sublist in tags for tag in sublist])\n",
    "\n",
    "def analice(ans):\n",
    "    ans=ans.split(', ')\n",
    "    \n",
    "    return str([i for i in ans if i in all_tags])\n",
    "\n",
    "df['chatgpt_tags'] = df['chatgpt_tags'].apply(analice)\n",
    "\n",
    "df.to_csv('../data/codeforce_chatgpt.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n de los resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [interactive, implementation, greedy]\n",
       "1         [greedy, implementation, strings]\n",
       "2             [math, number theory, graphs]\n",
       "3             [math, number theory, graphs]\n",
       "4        [greedy, implementation, sortings]\n",
       "                      ...                  \n",
       "95             [greedy, implementation, dp]\n",
       "96                 [greedy, implementation]\n",
       "97             [greedy, implementation, dp]\n",
       "98      [dp, trees, greedy, implementation]\n",
       "99             [greedy, implementation, dp]\n",
       "Name: chatgpt_tags, Length: 100, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/codeforce_chatgpt.csv')\n",
    "\n",
    "df['tags'].apply(ast.literal_eval)\n",
    "df['chatgpt_tags'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Accuracy: 0.02\n",
      "F1 Score (macro): 0.0024305555555555556\n",
      "F1 Score (micro): 0.02\n",
      "F1 Score (weighted): 0.017555555555555557\n",
      "F1 Score (samples): 0.0024305555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Etiquetas verdaderas y predichas (ejemplo)\n",
    "y_true = df['tags'].tolist()  # Etiquetas verdaderas\n",
    "y_pred = df['chatgpt_tags'].tolist()  # Etiquetas predichas por ChatGPT\n",
    "\n",
    "# Tag Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# F1 Score (macro)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# F1 Score (micro)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "# F1 Score (weighted)\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "f1_samples = f1_score(y_true, y_pred, average=None)\n",
    "f1_samples_avg = f1_samples.mean()\n",
    "\n",
    "print(\"Tag Accuracy:\", accuracy)\n",
    "print(\"F1 Score (macro):\", f1_macro)\n",
    "print(\"F1 Score (micro):\", f1_micro)\n",
    "print(\"F1 Score (weighted):\", f1_weighted)\n",
    "print(\"F1 Score (samples):\", f1_samples_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
